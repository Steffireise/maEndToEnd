---
title: "MA end to tend workflow"
author: "Bernd Klaus"
output: 
    BiocStyle::html_document:
        toc: true
        highlight: tango
vignette: >
    %\VignetteIndexEntry{MA end to tend workflow}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}  
---


<!--
To compile this document
graphics.off();rm(list=ls());rmarkdown::render('MA-Workflow.Rmd');
-->

```{r options, include=FALSE}
library(knitr)
options(digits=3, width=80)
opts_chunk$set(echo=TRUE,tidy=FALSE,include=TRUE,
               dev='png', fig.width = 6, fig.height = 3.5, comment = '  ', dpi = 300,
		cache = TRUE, lazy.load = FALSE )
```



# Required packages and other preparations




```{r required packages and data, echo = TRUE}
library(BiocStyle)
library(oligo)
library(geneplotter)
library(ggplot2)
library(dplyr)
library(LSD)
library(gplots)
library(RColorBrewer)
library(ArrayExpress)
library(arrayQualityMetrics)
library(stringr)
library(matrixStats)
library(topGO)
library(genefilter)
library(biomaRt)
library(pd.hugene.1.0.st.v1)
library(hugene10sttranscriptcluster.db)
library(pheatmap)
library(mvtnorm)
library(DAAG)
library(multcomp)
library(limma)
library(EnrichmentBrowser)
library(xlsx)

rawDataDir <- file.path(getwd(), "rawDataMAWorkflow")
```


In this document we introduce a complete workflow for a typical (Affymetrix) microarray
analysis. Data import, preprocessing, differential expression  and
enrichment analysis are discussed. We also introduce some necessary mathematical
background along the way.

The data used is from  [Palmieri et. al.](http://europepmc.org/abstract/MED/25901971),
a paper studying the differences between inflamed and noninflamed
colonic mucosa between patients suffering from Ulcerative colitis (UC) or 
Crohn's disease (CD).  This is a typical clincal data set consiting of 15 CD and
14 UC patients.


# Download the raw data from from ArrayExpress

The first step of the analysis is to download the raw data CEL files. Theses files
are produced by the array scanner software and contain the probe intensities 
measured. The data has been deposited at [ArrayExpress](https://www.ebi.ac.uk/arrayexpress/)
and has the accession code **E-MTAB-2967**. This accession code is commonly
reported in the original publication. 

Each Array--Express data set has a landing page summarizing the data set, 
the one for our data is here:

* [Data fromPalmieri et. al. at ArrayEpress](https://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-2967/)

We use the `r Biocpkg("ArrayExpress") ` Bioconductor package to obtain the links to
download the raw data.

# Information stored in ArrayExpress

In the repository for each dataset ArrayExpress stores a MAGE-TAB document with standardized
format. A MAGE-TAB document contains upt to five different types of files
Investigation DescriptionFormat (IDF), Array Design Format (ADF),
Sample and Data Relationship Format (SDRF), the raw data files and
the processed data files.

The Investigation Description Format (IDF) file contains top level information
about the experiment including title, description, submitter contact details and
protocols. The SDRF (Sample and Data Relationship Format) file containes
the information on the samples. 




# Get the raw and the annotation data 

Wih the code below, we download the raw data from
[ArrayExpress](https://www.ebi.ac.uk/arrayexpress/).

It is saved in the directory **rawDataDir** which defaults to the current
working directory. The names of the downloaded files are returned as a list.

```{r getDataEBI, eval=TRUE }
rawDataDir <- rawDataDir

if(!dir.exists(rawDataDir)){

    dir.create(rawDataDir)
}

annoAE <- getAE("E-MTAB-2967", path=rawDataDir, type="raw")
```

We now download the SDRF file directly to
in order to obtain the sample annotation.

The raw data constists of one CEL file per samples (see below) and
we use the .CEL file names as row names. They are given in a column
named `Array.Data.File` in the SDRF table. We turn the table into an
` AnnotatedDataFrame ` from the `r Biocpkg("Biobase") ` package that 
will need later to create an `ExpressionSet` for our data.


```{r getSDRF}
SDRF <-  read.delim(url("http://www.ebi.ac.uk/arrayexpress/files/E-MTAB-2967/E-MTAB-2967.sdrf.txt"),
                    row.names = "Array.Data.File")
SDRF <- AnnotatedDataFrame(SDRF)
```


Before we move on to the actual data import, we ill briefly introduce this
` ExpressionSet ` class contained in the `r Biocpkg("Biobase")` package.
It is commonly used to store Micorarray data in Bioconductor.

## Bioconductor ExpressionSets

Genomic data can be very complex,
usually consisting of a number of different bits and pieces.
In Bioconductor the approach is taken that these  pieces should be stored in
a single structure to easily manage the data.

The package `r Biocpkg("Biobase")` contains standardized data structures
to represent genomic data. The `ExpressionSet` class is designed
to combine several different sources of information into a single convenient
structure. An ExpressionSet can be manipulated (e.g., subsetted, copied),
and is the input to or output of many Bioconductor functions.

The data in an ExpressionSet consist of


+ **assayData**: Expression data from microarray experiments
(assayData is used to hint at the methods used to access different data
components).

+ **metaData**: A description of the samples in the experiment
(phenoData), metadata about the features on the chip or technology used for the
experiment (featureData), and further annotations for the features, for example
gene annotations from biomedical databases (annotation).

+ **experimentData**: A flexible structure to describe the experiment.



The ExpressionSet class coordinates all of these data, so that you do not usually
have to worry about the details. However, an ExpressionSet needs to be created in
the first place, because it will be the starting point for many of the analyses
using Bioconductor software.

```{r sumexp, echo=FALSE, fig.show="asis"}
par(mar=c(0,0,0,0))
plot(1,1,xlim=c(0,100),ylim=c(0,100),bty="n",
     type="n",xlab="",ylab="",xaxt="n",yaxt="n")
polygon(c(45,80,80,45),c(10,10,70,70),col=rgb(1,0,0,.5),border=NA)
polygon(c(45,80,80,45),c(68,68,70,70),col=rgb(1,0,0,.5),border=NA)
text(62.5,40,"assay(s)", cex = 1)
text(62.5,30,"e.g. 'exprs'", cex = 1)
polygon(c(20,40,40,20),c(10,10,70,70),col=rgb(0,0,1,.5),border=NA)
polygon(c(20,40,40,20),c(68,68,70,70),col=rgb(0,0,1,.5),border=NA)
text(30,40,"featureData", cex = 1)
polygon(c(45,80,80,45),c(75,75,90,90),col=rgb(.5,0,.5,.5),border=NA)
polygon(c(45,47,47,45),c(75,75,90,90),col=rgb(.5,0,.5,.5),border=NA)
text(62.5,82.5,"phenoData", cex = 1)
```

You can use the functions ` pData ` and ` fData ` to extract
the sample and feature annotation respectively from an ` ExpressionSet `.
The function ` exprs ` will return the expression data itself as a matrix.



## Import of microarray data and initial quality control

### Importing CEL files
The microrarray analysis of Affymetrix arrays starts with CEL files. These are the result
of processing of the raw image files using the Affymetrix software and contain
estimated probe intensity values. Each CEL file contains data on the intensity at each
probe on the chip, as well as some other quantities.
We collect the information about the CEL files
we want to import and then save them in  the variable `rawData`.

The followng code results in a character vector with the paths to the
CEL files we want to import. the function ` read.celfiles `
from the `r Biocpkg("oligo") ` can be
used to import the files. The package automatically uses
`r  Biocannopkg("pd.hugene.1.0.st") ` as the chip annotation package.

We specifiy our `AnnotatedDataFrame` created earlier as `phenoData` and check
whether the object created is valid. (e.g. sample names match between the different
tables)

```{r importCelfiles, results="hide", eval=TRUE, dependson="getSDRF"}
celfolder <- rawDataDir
cels <- list.celfiles(path = celfolder, pattern="*.CEL")

rawData <- read.celfiles(filenames = file.path(rawDataDir, cels),
                         verbose = FALSE, phenoData = SDRF)

validObject(rawData)
```

We now inspect the raw data a bit, remove some uninteresting phenoData columns 
and finally retain only those columns that are related to the experimental factors.
(IDs of the individuals, disease of the individual and the mucosa type)

```{r inspectPhenoData, results='hide', eval=TRUE }
head(pData(rawData))
head(exprs(rawData))
stopifnot(validObject(rawData))

pData(rawData) <- pData(rawData)[, c("Source.Name",
                                     "Characteristics.individual.",
                                     "Factor.Value.disease.",
                                     "Factor.Value.phenotype." )]
```


### Quality control of the raw data

The package `r Biocpkg("arrayQualityMetrics") ` produces an html report, 
containing lots of quality control plots together with a description of their
aims. We also check our raw data using this reporting tool.
Note that it cannot use more than 10 phenoData columns.


```{r arrayQualityMetricsRaw, eval=FALSE}
arrayQualityMetrics(expressionset = rawData,
    	outdir = "Report_for_Palmieri_raw",
			force = TRUE,
			do.logtransform = TRUE,
			intgroup = c("Factor.Value.disease." ,
			             "Factor.Value.phenotype.")
			)
```

The `r Biocpkg("arrayQualityMetrics") ` report shows that the second principal
component seems to differentiate  between the diseases. The density-- and
boxplots show that the  intensity distributions
of the individual arrays are quite different, indicating the need of an appropriate
normalization, which we will discuss next.


Basend on the PCA plot we remove the arrays belonging to individuals 255, 2209,
2826, 3262, 3302 and 3332 since arrays that are based on tissues from them have been
identified as outliers by at least one method or have an unusual intensity
distribution (255 and 2209) compared to the other arrays.


```{r removeOutliers, dependson="inspectPhenoData"}
outlierIdx <- pData(rawData)$Characteristics.individual. %in% c( 255, 2209,
                                                                  2826, 3262,
                                                                  3302,3332)
table(outlierIdx)

rawData <- rawData[, !outlierIdx ]
```


# Background adjustment, calibration, summarization and annotation


## Within--array normalization
After the initial import and quality assessment, the next step in processing of
microarray data is background adjustment. This  is essential because part of the
measured probe intensities are due to non-specific hybridization and the noise in the optical
detection system. Observed intensities need to be adjusted to give accurate measurements
of specific hybridization.

## Across--array normalization
Without proper normalization, it is impossible to compare measurements from
different array hybridizations due to many obscuring sources of variation.
These include different efficiencies of
reverse transcription, labeling, or hybridization
reactions, physical problems with the arrays, reagent batch effects, and laboratory
conditions. By looking at the box-- and densityplots of the array quality
metrics report, we can clearly see that the raw arrays are quite different from
each other.

## Summarization
After normalization,  summarization is needed because transcripts are represented
by multiple probes. For each gene, the background adjusted and normalized intensities
need to be summarized into one quantity that estimates an amount proportional to
the amount of RNA transcript.

After the summarization step, the summarized data can be annotated with various
information, e.g. gene symbols and EMSEMBL ids. The annotation database
contains the annotation information
for our platform. It is available via the package
\Biocannopkg{package:hugene10sttranscriptcluster.db}.

You can view its content like this

```{r annotation data base content, eval = TRUE}
head(ls("package:hugene10sttranscriptcluster.db"))
```

Additional information is available form the reference manual of the package.
Essentially, the package provides a mapping from the manufacturer ids to
the annotation.

## Old and new "probesets" of  Affymetrix microarrays

Traditionally, Affymetrix arrays (the so--called 3' IVT arrays)
were probeset based: a certain fixed group of probes were part of a probeset
which represented a certain gene.
(However a gene can be represented by multiple probesets)

The modern Gene and Exon arrays are exon based and hence there are two levels
of summarization. The exon level summarization is called a "probeset". However,
this probeset is not related at all to the  probesets of the old
chips, which usually represent a gene as mentioned above. Furthermore, there
are also no longer designated match/mismatch probes present on the chip.

For the newer Affymetrix chips a gene level summary is given by "transcript
clusters". Hence our annotation data base package is called
\Biocpkg{hugene10sttranscriptcluster.db}.

To make things even a bit more complicated the "Gene"  arrays  were created
as cheap versions of the "Exon" arrays by taking the "good"  probes from the Exon
array. So the notion of a probeset is based on the
original construction of the probesets on the Exon array, which contains
usually at least 4 probes.

But since Affymetrix only took the "good" probes for the Gene arrays, a lot of the
probesets on the Gene ST arrays are made up of 3 or fewer probes. Thus,
a summarization on the probeset / Exon level is not recommended for gene
arrays but still possible by using the \Biocannopkg{hugene10stprobeset.db} database.

## One--go preprocessing in oligo
`r Biocpkg("oligo") ` allows us to perform background correction, normalization
and summarization in one single step using a deconvolution method for
background correction, quantile normalization and
the RMA (robust multichip average) algorithm for summarization.

This package of algorithms as a whole is commonly called RMA algorithm,
although strictly speaking RMA is only a summarization method.

```{r RMAcalibration, eval=TRUE}
palmieriEset <- oligo::rma(rawData, target="core")
#save(palmieriEset, file = "palmieriEset.RData")
```

The parameter `target` defines the degree of summarization, the
default option "core", which contains transcript clusters containing
"safely" annotated genes. Other options for `target` include "extended"
and "full". For summarization on the exon level (not recommended for Gene
arrays), one can use "probeset" as the target option.

Other methods for background correction and normalization can be used as well,
however RMA is usually a good default choice.
RMA shares information across arrays and
uses an "aggressive" normalization method called quantile normalization that
will almost always "work", however it is preferable to apply it only after
outliers have been removed. The quantile normalization algorithm used by the
\Rfunction{rma} works by replacing values for  by the average of identically
ranked values across arrays. A more detailed description can be found on the
wikipedia page: \url{https://en.wikipedia.org/wiki/Quantile_normalization}.

An alternative to quantile normalization is the `r Biocpkg("vsn") ` algorithm,
that preforms background correction and normalization by robustly
shifting and scaling log--scale intensity values within arrays.

## Some mathematical background on normalization/calibration
and background correction}
A generic model for the value of the intensity $y$ of a single probe on
a microarray is given by

$$
    Y = B + \alpha \cdot S
$$
    
where B is a random quantity due to background noise, usually composed of
optical effects and non-specific binding, $\alpha$ is a gain factor, and $S$
is the amount of measured specific binding. The signal $S$ is considered a
random variable as well and accounts for measurement error and probe effects.
The measurement error is typically assumed to be multiplicative so we write:

$$
    \log(S) = \vartheta + \varphi + \varepsilon
$$
    
Here $\vartheta$ represents the logarithm of the true abundance,
$\varphi$ is a probe-specific effect, and $\varepsilon$ accounts for measurement error.
This is the additive--multiplicative error model for microarray data used by RMA
and also the `r Biocpkg("vsn") ` algorithm. They differ in the way $B$ is removed and
an estimate of $\vartheta$ is obtained.



## Quality assessment of the calibrated data
After running the RMA algorithm it is a good idea to produce another
quality control report.

```{r array Quality metrics calibrated, eval=FALSE}
#load(file.path("dataMAWorkflow", "palmieriEset.RData"))

arrayQualityMetrics(expressionset = palmieriEset,
    outdir = "Report_for_Palmieri_Calibrated",
    force = TRUE,
    do.logtransform = FALSE,
    intgroup = c("Factor.Value.disease." ,
                "Factor.Value.phenotype.")
)
```


Additional potential outliers are identified, however they are close to the other
samples on the PCA plot, so we keep them.

We also produce another clustering and PCA plot. In order to display a
heatmap of the sample--to--sample distances, we first compute them using
the `dist` function. We need to transpose the expresssion values since
the function computes the distances between the rows (i.e. genes in our case) by
default. The distance chosen is the Eucledian one, however this can of course be
changed and we choose the manhatten distance here which uses absolute instead of
squared distances. We se the diagonal of the distance matrix to NAs to increase
the contrast. Those diagonal entries do not contain information since the
distance of a sample to itself is always equal to zero.
We then cluster the sample to sample distances again using the manhattan distance
to se whether blocks appear in the matrix.


```{r PCAMetricsCalibrated, fig.height = 4.5, fig.cap="PCA plot and clustering of the calibrated data", eval = TRUE }
#load(file.path("dataMAWorkflow","palmieriEset.RData"))

expPalmieri <- exprs(palmieriEset)
PCA <- prcomp(t(expPalmieri), scale = FALSE)

dataGG <- data.frame(PC1 =PCA$x[,1], PC2 = PCA$x[,2],
                    disease = pData(palmieriEset)$Factor.Value.disease.,
                    phenotype = pData(palmieriEset)$Factor.Value.phenotype.
          )
(qplot(PC1, PC2, data = dataGG, color =  disease, shape =  phenotype,
       main = "Raw Data", size = I(4), asp = 1.0  )
 + scale_colour_brewer(palette = "Dark2"))

dists <- as.matrix(dist(t(expPalmieri), method = "manhattan" ))
diag(dists) <- NA
rownames(dists) <-  pData(palmieriEset)$Factor.Value.disease.
hmcol <- colorRampPalette( rev(brewer.pal(9, "PuOr")) )(255)


pheatmap(dists, col = rev(hmcol), clustering_distance_rows = "manhattan",
                                 clustering_distance_cols = "manhattan")
```

The second PC now roughly separates Chron's disease from ulcerative colitis. On
the heatmap plot we also see that the samples cluster by disease but not
perfectly so, confirming the impression from the PCA plot that the separation
between the diseases is not perfect.



## Filtering based on intensity 
We now filter out lowly expressed genes. Microarrays commonly show a large number
of probes in the background range. They do not change much and have a low
intensity. Hence they combine a low variance with
a low intensity. Thus they could end up as being detected as differentially
expressed although they are barely above the "detection" limit. We will perform
a "soft" intensity based filtering here, since this is recommended by
the `r Biocpkg("limma") ` although a variance based filter might filter a similar
set of probes in practice. We filter low gene
medians by visually inspecting fitting a "null" distribution
$0.05 \cdot N(5.1, 1.18)$ of the gene--wise medians representing their typical
behavior.

Then we use the 5% quantile of this distribution as a threshold. We keep
only those genes that show an expression higher than threshold in at least
as many arrays as in  the smallest experimental group.

```{r expGroups, dependson="PCAMetricsCalibrated"}
table(pData(palmieriEset)$Factor.Value.disease.)
```

In our case this would be \Sexpr{table(pData(palmieriEset)$Factor.Value.disease.)[1]}.


```{r intensityBasedFiltering, fig.width=10, fig.height=6, eval=TRUE}
PalmieriMedians <- rowMedians(exprs(palmieriEset))
tmp <- hist(PalmieriMedians, 100, col="lavender", freq = FALSE)

emp_mu <- tmp$breaks[which.max(tmp$density)]
emp_sd <- mad(PalmieriMedians)/2
eta0 <- 0.50

lines(sort(PalmieriMedians), eta0*dnorm(sort(PalmieriMedians),
  			mean = emp_mu , sd = emp_sd ),
				col = "darkorange", lwd =4)

### find 5% quantile of fitted null model
cut_val <- 0.05 / eta0
thresh_median <- qnorm(0.05 / eta0, emp_mu, emp_sd)


noOfSamples <- table(pData(palmieriEset)$Factor.Value.disease.)[1]
idx_thresh_median <- apply(exprs(palmieriEset), 1, function(x){
	sum(x > thresh_median)  >  noOfSamples
    })
    table( idx_thresh_median)


PalmieriFiltered <- subset(palmieriEset, idx_thresh_median )
```





## Annotation of the transcript clusters

Before we continue with the linear models for microarrays and differential
expression  we  describe how to add ``feature Data", i.e. annotation
information to the transcript clusters. We use the \Rfunction{select}
from `r Biocpkg("AnnotationDbi") ` to query the gene symbols and associated
short descriptions for the transcript clusters.



```{r annotateData, eval=TRUE, dependson="intensityBasedFiltering"}
annoPalmieri <- AnnotationDbi::select(hugene10sttranscriptcluster.db,
                                  keys=(featureNames(PalmieriFiltered)),
                                  columns = c("SYMBOL", "GENENAME"),
                                  keytype="PROBEID")
```

## Removing multiple mappings and building custom annotations

Many transcript--cluster IDs will map to multiple gene symbols.
We compute a summary table in the code below to see how many there are.

```{r multipleMappings, dependson="annotateData"}

probeStats <- annoPalmieri  %>%
    group_by(PROBEID) %>%
    summarize(noOfMatches = n_distinct(SYMBOL)) %>%
    filter( noOfMatches > 1)

probeStats

dim(probeStats)

```

We have  over 2000 transcript--clusters that map to multiple gene symbols.
It is diffcult to decide which mapping is "correct". Therefore,
we exclude these transcript--clusters. Additionally, we also exclude
transcript--clusters that do not map to gene symbols.


```{r excludeMultipleMappings, dependson="multipleMappings"}

IDsToExlude <- ((featureNames(PalmieriFiltered) %in% probeStats$PROBEID) |
               featureNames(PalmieriFiltered)  %in% subset(annoPalmieri,
                                                           is.na(SYMBOL))$PROBEID)
table(IDsToExlude)

PalmieriFinal <- subset(PalmieriFiltered, !IDsToExlude )


fData(PalmieriFinal)$PROBEID <- rownames(fData(PalmieriFinal))
fData(PalmieriFinal) <- left_join(fData(PalmieriFinal), annoPalmieri)

```

Alternatively, one can re--map the probes of the array
to a current annotation, a workflow to do to this for Illumina arrays is given in

[Arloth et. al.](https://dx.doi.org/10.1371/journal.pone.0139516)

Essentially, the individual probe sequences are re--aligned to an in--silico
"exome" that consists of all annotated transcript exons.

In any case, the package `r Biocpkg("pdInfoBuilder") ` can be used to build custom
annotation packages for use with `r Biocpkg("oligo") `. In order to do this,
PGF / CLF files  (named ``Library files'' on the Affymetrix website) as well
as the probeset annotations are required. The probesets typically represent
a small streches of the genome (such as a single exon) and multiple probesets
are then used to form a transcript cluster.

The CLF file contains information about the location of
individual probes on the array. The PGF file then contains the individual probe
sequences and shows the probeset they belong to. Finally, The probset annotation .csv
then finally contains information about which probesets are used
in which transcript cluster. Commonly, multiple probesets are used in one
transcript cluster and some probesets are contained in multiple transcript
clusters.


# A short overview of linear models

I am afraid this section is rather technical. However general experience shows that
most questions on the internet regarding packages on Bioconductor using linear models
like `r Biocpkg("limma") `,  `r Biocpkg("DESeq2")` and `r Biocpkg("edgeR") ` are actually
not so much about the packages themselves but rather about the underlying linear
models. It might also be helpful to learn a bit of linear algebra to understand
the concepts better. The Khan Academy [offers nice (and free) online courses](https://www.khanacademy.org/math/linear-algebra).

Mike Love's and Michael Irizzary's [genomics class](http://genomicsclass.github.io/book/)
is also a very good resource, especially its section on [interactions and contrasts](http://genomicsclass.github.io/book/pages/interactions_and_contrasts.html).

## Regression models 

In regression models we use one variable to explain or predict the other. It is
customary to plot the predictor variable on the x--axis and the predicted variable on
the y-axis.
The predictor is also called the independent variable, the explanatory variable, the
covariate, or simply x. The predicted variable is called the dependent variable, or
simply y.

In a regression problem the data are pairs $(x_i , y_i )$ for $i = 1, \dotsc , n$.
For each $i, y_i$ is a random variable whose distribution depends on $x_i$. We write

$$
y_i = g(x_i) + \varepsilon_i .
$$

The above  expresses $y_i$ as a systematic or explainable part $g(x_i)$ and an unexplained part
$\varepsilon_i$. Or more informally: response = signal + noise.
$g$ is called the regression function. Often the  goal is to estimate $g$. As
usual, the most important tool to infer a suitable function is a simple scatterplot. Once we have an estimate $\hat g$ of $g$, we can compute $r_i := y_i - g(x_i )$. The $r_i$ 's are called residuals.
The $\varepsilon_i$'s themselves are called errors.

Residuals are used to evaluate and assess the fit of models for g. Usually one makes distributional
assumption about them, e.g. that they are i.i.d. normally distributed with identical variance
$\sigma^2$ and mean zero:

$$
r_i \sim N(0, \sigma^2)
$$

This allows to derive statistical tests for model coefficients.

## Linear regression models

Linear regression is a special case of the general regression model. Here,
we combine the predictors linearly to produce a prediction. If we have only
single predictor $x$, the simple linear regression model is:

$$
y_i = \beta_0 +  \beta_1 x_{ij} + \varepsilon_i; \quad
 \varepsilon_i \sim N(0, \sigma);
$$

We can of course always add more predictors, let their total number be denoted by
$p$. Then we get a multiple linear regression:

$$
y_i= \beta_0 + \sum_{j=1}^p \beta_j x_{i} + \varepsilon_i; \quad  j=1,...,p
$$

The equation for a multiple linear regression model can also be written in matrix
form. (We will also denote matrices and vectors in bold font).

$$
\bY_{n \times 1} = \bX_{n \times (p+1)}  \bbeta_{(p+1) \times 1} + \bvarepsilon_{n \times 1}
$$

With $\mathbf{X}$ being the so called **design matrix**

$$
\mathbf{X}:=  \bigl( \mathbb{1}_n, X_1, \dotsc, X_p\bigr)
$$

$\mathbb{1}_n$ is a columns vector of ones called the **intercept** and 
X_p = (x_{p1}, \dotsc, x_{pn})^T
is a column vector of measurements for covariate $p$. The regression
coefficients are commonly estimated by
the method of ``ordinary least squares" (OLS)}
\text{OLS:}  \bigl|\bigl| Y - \beta_0 - \sum_{j=1}^p \beta_j X_{j} \bigl|\bigl|_E \rightarrow \min!
leading to the estimate of the coefficient vector}
\hat \beta = ( \bX^T \bX)^{-1} \bX^TY
An important special case is when the design is "orthogonal", i.e.
the covariates are "uncorrelated"
and $( \bX^T \bX)^{-1} = \text{Diag}(c_1, \dotsc c_p) $ is a diagonal matrix
with some values $c_j$.
This is commonly the case for a design with no intercept. Note that the
covariates are modeled
as fixed quantities, and are not  centered so $\bX^T \bX$ is not actually a
covariance  matrix. In this case the estimate of the coefficient vector becomes}
\hat \beta =  \text{Diag}(c_1, \dotsc c_p)  \bX^TY = \bigl(X_1, \dotsc, X_p\bigr)^TY


So that the fitted values for individual coefficients $\hat \beta_j, \; j \in
\{0, \dotsc, p\}$ are independent of all the other coefficients. This  is
a very desirable feature and also leads to computational efficiency: Instead
of fitting a multiple regression, we can as well fit a simple regression for each
of the covariates separately.



##Creating design matrices in R

Parts of this section are based on material by Mike Love.
To get an idea of what design matrices look like, we consider several examples.
It is important to know some fundamentals
about design matrices in order to be able to correctly transfer a design of a
particular study to an appropriate linear model.

We will use the  base R functions:

*  ` formula `
*   ` model.matrix `


...in order to produce design matrices for a variety of linear models. R uses the formula
interface to create design matrices automatically. In the first example, we have
two groups of 2 samples each. Using \Rfunction{formula}  and  \Rfunction{model.matrix}
we will create  a model matrix with so called treatment contrast parameterization (the
default setting in R). This means
that an intercept is included in the model, i.e. $X_0 = \mathbb{1}_n$
and $X_1$ is equal to 1 if the samples belongs to group two and zero otherwise
as we can the see in the R output.


```{r simple treatment contrast}
two.groups <- factor(c(1,1,2,2))
f <- formula(~ two.groups)
model.matrix(f)
#model.matrix(f, contrasts.arg=list(two.groups="contr.sum", contrasts = FALSE))
```

This design is called treatment contrast parameterization for an obvious reason:
the first column of the design matrix represents a ``base level", i.e the  mean $\beta_0$
for group one and  the second column, corresponding to $\beta_1$,
represents the difference between
the group means since all group two samples have means $\beta_0 + \beta_1$. Since
$\beta_0$  is the mean of group 1, $\beta_1$ must correspond to the difference
of the means of group two and group one and thus shows the effect of a "treatment".

However, this design is not orthogonal. We can construct an equivalent
orthogonal design as follows:

```{r simple orthogonal contrast}
gr1 <- ifelse(two.groups == 1, 1,0)
gr2 <- ifelse(two.groups == 2, 1,0)
orth.model <- model.matrix(~ 0 + gr1 + gr2)
#crossprod(orth.model,orth.model)
```

Here, we loose the nice direct interpretability of the coefficients. Now
$\beta_1$ is simply the mean of the second group. We will discuss the extraction
of interesting contrasts from a model like this below.

We explicitly excluded the intercept by specifying it as zero.
Commonly it makes sense to include an intercept in the model, especially
in more complex models.

We can also specify a more complex design easily: if we have two independent
factors, the base mean now corresponds to the first levels of the two factors.

```{r  basic complex design}
x <- factor(c(1,1,1,1,2,2,2,2))
y <- factor(c("a","a","b","b","a","a","b","b"))
two.factors <- model.matrix(~ x + y)
two.factors
```

The ``drop one" strategy is the default method for creating regression coefficients
from factors in R. If a factor has $d$ levels, adding it to the model will give
you $d-1$ regression coefficients corresponding to  $d-1$ columns in a design
matrix.

We can also include factor interactions easily using the $*$ and $:$ operators.
The $*$ operator stands for main effects and interaction, while the  $:$ operator
means interaction only.
An interaction is modeled as a multiplication of the estimated coefficients.
The following two commands create the same interaction models.

```{r  interaction designs }
interaction.model.1 <- model.matrix(~ x + y + x:y)
interaction.model.2 <- model.matrix(~ x*y)

interaction.model.1
interaction.model.2
```

Apart from excluding the intercept, you can also use the \Rfunction{I} function
to treat a covariate as it is without using the formula syntax.
The code below includes $z^2$ as covariate.

```{r other design related features}
z <- 1:4
model.matrix(~ z)
model.matrix(~ 0 + z)
model.matrix(~ z + I(z^2))
```

##Singular model matrices}

No matter how your  model matrix looks like, you should make sure that it is
non--singular. Singularity means that the measured variables
are linearly independent and leads to
uniquely defined regression coefficients. In linear algebra terms, we say that
the matrix does not have full rank, which for design matrices means that
the actual dimension of the space spanned by the column vectors is in fact lower
than the apparent one. This leads to a redundancy in the model matrix, since
some columns can be represent by linear combinations of other columns.

For factor models, this  happens  if two conditions are confounded,
e.g. in one experimental
group there are only females and in the other group there are only males.
Then the effect of sex and experimental group cannot be disentangled.

Let's look at an example. We set three factors, of which the
third one is nested with the first two. We can check the singularity of
the model matrix by computing its so called singular value decomposition and check
it's minimal singular value. If this is zero, the matrix is singular.
As we can see, this is indeed the case here.

```{r singular model matrix}
x <- factor(c(1,1,1,1,2,2,2,2))
y <- factor(c("a","a","b","b","a","a","b","b"))
z <- factor(c("m","m","m","m","k","k","l","l"))
sing.model <- model.matrix(~ x+y+z)
sing.model
round(min(svd(sing.model)$d))
```


Here the we have one column in the design matrix that can be represented by
a linear combination of the other columns, thus the column space has actually
a lower dimension than the apparent one. For example, we can represent column
5 ("zm") by a linear combination of the first two columns "intercept" and "x2":

```{r singular model matrix lm}
comb.coefs  <- coef(lm(sing.model[,5] ~ 0 + sing.model[,-5]))
comb.coefs <-  ifelse(is.na(comb.coefs), 0L, comb.coefs)
comb.coefs
round(sum(sing.model[,-5] %*% comb.coefs - sing.model[,5]))
```

I.e., in mathematical notation this means
$$
    \begin{pmatrix}
        1\\1\\1\\1\\0\\0\\0\\0
    \end{pmatrix}
    = 1\cdot 
    \begin{pmatrix}
        1\\1\\1\\1\\1\\1\\1\\1 
    \end{pmatrix}
    -1\cdot 
    \begin{pmatrix}
        0\\0\\0\\0\\1\\1\\1\\1
    \end{pmatrix}
$$

Thus the corresponding regression coefficients are not uniquely determined
and the model does not make much sense. The function \Rfunction{lm} handles a
singular design matrix quite intelligently, it will remove a dependant column.
We now explore this by  simulating some normally distributed random numbers and
fitting a model to this data using our matrix.

We can specify the model matrix directly by specifying on the right hand side
of the tilde ind the \Rfunction{lm}. R correctly notices that
the m category of $z$ and group 1 of $x$ are the same and sets the corresponding
coefficient to NA.

```{r singularmodel for factors data and fit}
lm(rnorm(8) ~ 0 + sing.model)
```

This is also true if  we have numerical dependent data, as in the following
example:

```{r singularmodel for real valued  data and fit}
sing.real <- data.frame(x = t(rmvnorm(1, mean= sort(rep(8:9,4)))),
                       y = t(rmvnorm(1, mean= rep(c(rep(4,2), rep(5,2)),2))))
sing.real <- mutate(sing.real, z = y-2*x)
min(svd(sing.real)$d)
round(sing.real <- mutate(sing.real, target = x + y + z + rnorm(8)))

lm(target ~ x + y+ z, data = sing.real)
```

R removes z, which is just a linear combination of x and y.
Note however, that the ordinary least squares fit will
always "work", whether the model matrix is singular or not. This might
give results requiring careful interpretation in some cases. E.g., there
is nothing special about z. We could have excluded
y or x as well instead of z.

Therefore, the non--singularity of the model matrix should
always be checked beforehand.


## Using contrasts to test hypotheses  in linear regression 

In differential expression analysis, our most important covariates will
be factors that differentiate between two or more groups, e.g. the
covariate $X_p = (\bx_{p1}, \dotsc, \bx_{pn})$ is either zero or one
depending on which group the sample belongs to.

We will illustrate this concept using a toy data set called toycars
from the `r CRANpkg("DAAG") `.
The data set `toycars` gives  the distance traveled by one of
three different toy cars on a smooth surface, starting from rest at
the top of a 16 inch long ramp tilted at varying angles. We have the
variables:

\begin{itemize}
\item `angle`: Angle
\item `distance`: Traveled distance
\item `car`: Car number (1, 2 or 3)
\end{itemize}

We transform car into a factor so that R performs the necessary parameterization
of the contrasts automatically.
```{r toycars_boxplot }
toycars$car = as.factor(toycars$car)
qplot(car, distance, data = toycars, geom="boxplot") + geom_point()
```

By looking at the box plots of distance by car, we can clearly see differences
between the three types of cars. We can now fit a linear model with
distance as the dependent variable and car and angle as the predictors. As we
can see from the linear model output, the treatment contrast parameterization
was used, with car 1 being the base level.

```{r toycars_linear_model }
lm.car <- lm(distance~angle+car, data = toycars)
summary(lm.car)
```


##Testing general linear hypotheses}

The estimated coefficients now give us the difference between the distance
traveled between car 1 and car 2 (0.11) and car 1 and car 3 (-0.08), and the associated
t--tests of these coefficients. However we cannot see a test of car 2 vs. car 3.
This contrast test would correspond to testing the difference between
the car2 and car3 regression coefficients.

Thus, contrasts of interest to us may not be readily correspond to coefficients
in a fitted linear model. However, one can easily test general linear hypotheses
of the coeffecients of the form:

$$
    H_0: \bC_{ q \times (p+1) } \bbeta_{ (p+1) \times 1 } = \balpha_{ q \times 1 }
$$

Where $\bC$ is the contrast matrix containing the between group differences
of interest, $q$ is the total number of comparisons to be performed and
$\balpha$ contains the difference to be tested, this is usually a vector
of zeros. If one tests multiple coeficients at once (e.g. $\beta_1 = 0$ and
$\beta_2 = 0$ the corresponding test statistic is $F$--distributed and if one just
tests linear combinations of coefficients, e.g.  $\beta_1 - \beta_2 = 0$, $\beta_1 - \beta_2 - 2\beta_3 = 0$ or something like that the test statistic
has t--distribution. The function \Rfunction{summary} reports the results of
$\beta_1 = \beta_2 = \dotso = \beta_p = 0$ (F--test) and the t--tests of $\beta_j = 0$
each of the coefficients.

Not that the model does not actually have to be refitted in order
to test the contrasts. This makes contrast matrix based testing
more efficient and convenient than reformulating the model
using a new paramerization of the factors to obtain the desired tests.

We can use the function \Rfunction{glht} from the `r CRANpkg("multcomp") `
to test these general linear hypotheses. Let us assume we want all pairwise
comparisons between the cars, this can be achieved by defining a contrast
specific matrix for our current model given below.

There are 3 such comparisons  and we can print
the results by using the \Rfunction{summary} function. As we can see the estimates
for the contrasts already contained in the original model agree with the obtained
results.

```{r toycars contrast test }
C <- rbind(c(0,0,1,0), c(0,0,0,1), c(0,0, 1,-1) )
colnames(C) <- rownames(summary(lm.car)$coefficients)
rownames(C) <- c("car2 - car1", "car3 - car1" , "car2 - car3" )
C
summary(glht(lm.car, C), test = adjusted(type ="none"))
```

Usually, the contrast matrix is assumed to have full rank.
The introduction of an intercept in our model creates a
redundancy that has to be resolved. Also, the metric covariate has
to be taken into account. The function \Rfunction{glht} does this automatically
for us. Commonly, one uses a model without an intercept and extracts
contrasts of interest as we will see later when using `r Biocpkg("limma") ` and in the
exercise below.

Note that the term "contrast" is  used in the context of (re)parameterization
of the original model (as in "treatment contrasts") and in the testing of
linear hypotheses about the model coefficients. This can lead to some confusion,
however usually it should be clear from the context whether a reparameterization
or test of linear hypotheses is intended.


#Linear models for microarrays

We now apply linear models to microarrays. Specifically, we discuss
how to use the `r Biocpkg("limma") ` for differential expression analysis.
The package is designed to analyze complex experiments involving comparisons between
many experimental groups simultaneously while remaining reasonably easy to use
for simple experiments.

The central idea is to fit a linear model to the expression data for each gene.
Empirical Bayes and other shrinkage methods are used to borrow information
across genes making the analyses stable even for experiments with small number of
arrays.

As introduced above, we use appropriate design and contrast matrices for
our linear models and fit a linear model to each gene separately.

##A linear model for the data
The original paper was interersted in changes in transcription that occur
between inflamed and adjacent noninflamed mucoasal areas of the colon
in both inflamatory bowel disease types studied.

Since we have two arrays per individual, the first factor we need
is a blocking factor for the individuals that will absorb differences between
them. Then we create a factor that gives us the combination of  disease and
the tissue type and allows use to identify differentially expressed genes between
the tissues types in the two diseases. We simpify the names of the
diseases. Then, we create two design matrices, one for each of the two diseaeses.

```{r createDesign, eval=TRUE, dependson="excludeMultipleMappings" }
#load(file.path("dataMAWorkflow", "PalmieriFinal.RData"))

individual <- as.character(pData(PalmieriFinal)$Characteristics.individual.)

tissue <- str_replace_all(pData(PalmieriFinal)$Factor.Value.phenotype., " ", "_")
tissue <- ifelse(tissue == "non-inflamed_colonic_mucosa", "nI", "I")

disease <- str_replace_all(pData(PalmieriFinal)$Factor.Value.disease., " ", "_")
disease <- ifelse(disease == "Crohn's_disease", "C", "U")


i <- individual[disease == "C"]
designPalmieriC <- model.matrix(~ 0  + tissue[disease == "C"]
                                + i)
colnames(designPalmieriC)[1:2] <- c("I", "nI")


i <- individual[disease == "U"]
designPalmieriU <- model.matrix(~ 0  + tissue[disease == "U"]
                                + i)
colnames(designPalmieriU)[1:2] <- c("I", "nI")

## test rank

```

We can now inspect the design matrices and test their rank.

```{r inspectDesingMatrix, eval = TRUE}
head(designPalmieriC[, 1:6])
dim(designPalmieriC)

head(designPalmieriU[, 1:6])
dim(designPalmieriU)
```


## Contrasts and hypotheses tests
We can now fit the linear model and
define appropriate contrasts to test hypotheses of interest.
We want to compare the inflammed  to the the non--inflammed tissue.
We create a contrast matrix consisting of one line. `r Biocpkg("limma") `s function
\Rfunction{makeContrasts}
does this for us. We can fit the linear model, compute the moderated $t$--statistics
by calling the \Rfunction{eBayes} function and finally extract the number of
differentially expressed genes while controlling the FDR by
requiring BH--corrected p--value below a certain threshold.

```{r createContrastMatrixAndFitModelFit, eval=TRUE, dependson="createDesign" }
contrastMatrixC <- makeContrasts(I-nI, levels = designPalmieriC)

PalmieriFitC <- eBayes(contrasts.fit(lmFit(PalmieriFinal[,disease == "C"],
                                design = designPalmieriC),
                                contrastMatrixC))

contrastMatrixU <- makeContrasts(I-nI, levels = designPalmieriU)

PalmieriFitU <- eBayes(contrasts.fit(lmFit(PalmieriFinal[,disease == "U"],
                                design = designPalmieriU),
                                contrastMatrixU))
```

## Extracting results
Results can be extracted by use of the \Rfunction{topTable} function. We extract
the comparisons for both Chron's disease  as well as  u and
sort the results by their $t$--statistics. As a diagnostic check, we also
plot the  $p$--value histogram: We excpect a uniform distribution for the
p--values that correspond to true null hypotheses, while the a peak near zero
shows a enrichment for low p--values corresponding to differentially expressed (DE)
genes.  A p--value less than 0.001 was used in the original paper as a significance
cutoff leading to 298 (CD) and 520 (UC) DE--genes for the two diseases.

We call around 1000 genes in both conditions at the same cutoff, this is probably
due to the increased power from the blocking according to the individuals
and the moderated variance estimation that `r Biocpkg("limma") ` performs.

```{r extractResults, eval = TRUE, dependson="createContrastMatrixAndFitModelFit"}
tableC <-  topTable(PalmieriFitC, number = Inf, sort.by = "t")

table(tableC$adj.P.Val < 0.05)

table(tableC$P.Value < 0.001)

hist(tableC$P.Value, col = brewer.pal(3, name = "Dark2")[1],
     main = "inflammed - not inflamed -- Crohn's disease")



tableU <-  topTable(PalmieriFitU, number = Inf)
head(tableU)

table(tableU$adj.P.Val < 0.05)

table(tableU$P.Value < 0.001)

hist(tableU$P.Value, col = brewer.pal(3, name = "Dark2")[2],
     main = "inflammed - not inflamed -- Ulcerative colitis")


```


## Comparion to the paper results

We now compare our list of differentially expressed genes to the results obtained
in the paper. The paper results can be downloaded as excel files  from
[this link](http://links.lww.com/IBD/A795). The paper results are  given
as a .xls file than contains lists of identified differentially expressed genes
with a p--value less tha 0.001, which corresponds to an FDR of 0.05 in Chron's
disease and 0.02 in ulcerative colitis. There are four tables in total giving
the list of up and downregulated genes in CD and UC respectively.
In the code below, we extract the gene symbols from the excel table and then
compare them to the differentially expressed genes we identify at a p--value
of 0.001.


```{r compareDEgenes, dependson="extractResults"}
#download.file("http://links.lww.com/IBD/A795", destfile = "PalmieriDERes.xls")
PalmieriDERes <- sapply(1:4, function(i) read.xlsx(colIndex = 1,
"PalmieriDERes.xls", sheetIndex = i, startRow = 4))

#load(file.path("dataMAWorkflow", "PalmieriDERes.RData"))

names(PalmieriDERes) <- c("CD_UP", "CD_DOWN", "UC_UP", "UC_DOWN")
PalmieriDERes <- lapply(PalmieriDERes, as.character)
paperDEgenesCD <- Reduce("c", PalmieriDERes[1:2])
paperDEgenesUC <- Reduce("c", PalmieriDERes[3:4])

overlapCD <- length(intersect(subset(tableC, P.Value < 0.001)$SYMBOL,  paperDEgenesCD)) /
    length(paperDEgenesCD)


overlapUC <- length(intersect(subset(tableU, P.Value < 0.001)$SYMBOL,  paperDEgenesUC)) /
    length(paperDEgenesUC)
```


We see that we get a moderate overlap of \Sexpr{overlapCD} for CD and
\Sexpr{overlapUC} for UC. Note that is recommendend to always to choose an
FDR cutoff instead of a p--value cutoff, since this way you control an
explicitly defined error rate and the results are easier to interpret and
to compare. In what follows, we choose an FDR cutoff of 5\%.


# Gene ontology enrichment analysis
We can now try characterize the identified differentially expressed genes
a bit better by performing an GO enrichment analysis. Essentially the
gene ontology (\url{http://www.geneontology.org/}) is hierarchically organized
collection of functional gene sets. For a nice introduction to the GO see

\begin{itemize}
  \item du Plessis et. al. - The what, where, how and why of gene ontology primer for bioinformaticians- Bioinformatics, 2011
\end{itemize}

### Matching the background set

The function \Rfunction{genefinder} from the `r Biocpkg("genefilter") ` will be used
to find background genes that are similar to the differentially expressed
genes. We then check whether the background has roughly the same distribution
of average expression strength as the foreground.

We do this in order not to select a biased background since the gene set testing
is performed by a simple Fisher test on a 2x2 table. Note that this appraoch
is very similar to web tools like DAVID. Here we focus on the  CD data.

For every differentially expressed gene, we try to find 5 genes with similar
expresssion. We then

```{r GOAnalysisCreateBackgrounds, eval=TRUE, dependson="extractResults", warning=FALSE}
DEgenesCD <- rownames(subset(tableC, adj.P.Val < 0.05))

#backGIdx <- genefinder(PalmieriFinal, DEgenesCD,  5,
#  method="manhattan", scale="none")
#backGIdx <- sapply(backGIdx, function(x)x$indices)

#backGIdx <- backGIdx[is.finite(backGIdx)]

#backG <-featureNames(PalmieriFinal)[backGIdx]
# backG <- setdiff(backG, DEgenesCD)
# length(backG)
# 
#   multidensity( list(
#        all=  tableC[,"AveExpr"] ,
#        fore= tableC[DEgenesCD, "AveExpr"],
#        back= tableC[rownames(tableC) %in% backG, "AveExpr"]),
#      xlab="mean expression",
#   main = "DE genes for CD - Background - Matching")
```

We can see that the matching returned a sensible result, we can now
perform the actual testing. For this purpose we use the `r Biocpkg("topGO") ` which
implements a nice interface to Fisher testing and also has additional algorithms
taking the GO structure into account, by e.g. only reporting the most specific
gene set in the hierarchy. For more details, see the paper:

\begin{itemize}
  \item Alexa et. al. - Improved scoring of functional groups from gene expression data
by decorrelating GO graph structure - Bioinformatics, 2006
\end{itemize}

The GO has three top ontologies, cellular component (CC), biological  processess
(BP), and molecular function (MF). Here we only use the biological  processess category
for illustration.

### Running topGO

We first create a factor `allG` which indicates for every gene in
our background / universe, whether it is differentially expressed or not.

```{r createFactorOfInterestingGenes, dependson="GOAnalysisCreateBackgrounds", eval=FALSE}

geneIDs <- rownames(tableC)
inUniverse <- geneIDs %in% c(DEgenesCD,  backG)
inSelection <-  geneIDs %in% DEgenesCD
allG <- factor( as.integer( inSelection[inUniverse] ) )
names(allG) <- geneIDs[inUniverse]

```

We now initialize the `r Biocpkg("topGO") ` data set, using the GO annotations contained
in the annotation data base for the chip we are using. The `nodeSize`
parameter specifies a minimum size of a GO category we want to use: i.e. here
categories with less than 10 genes are not included in the testing.

```{r createTopGODataSet, dependson="createFactorOfInterestingGenes", eval=FALSE }
ont <- "BP"

tgd <- new( "topGOdata", ontology=ont, allGenes = allG,
 nodeSize=10, annot=annFUN.db, affyLib="hugene10sttranscriptcluster.db" )
```

Now the tests can be run. `r Biocpkg("topGO") ` offers a wide range of options,
for details see the paper or the package vignette.

Here we run two common tests: an ordinary Fisher test for every GO category, and the
"elim" algorithm, which tries to incorporate the hierachical structure of the
GO and to "decorrelate" it.

The algorithm starts processing the nodes/GO category
from the highest (bottommost) level and then iteratively
moves to nodes from a lower level. If a node is scored as significant,
all of tis genes  are marked as removed in all ancestor nodes.
This way, the "elim" algorithm aims at finding the most specific node
for every gene.

The tests uses a 0.01 p--value cutoff by default.

```{r runtopGOTests, results='hide', eval=FALSE, dependson = "createTopGODataSet"}
resultTopGOElim <- runTest(tgd, algorithm = "elim", statistic = "Fisher" )
resultTopGOClassic <- runTest(tgd, algorithm = "classic", statistic = "Fisher" )
```

We can now look at the results, we look at the top 100 GO categories according
to the ``Fisher elim" algorithm. The function \Rfunction{GenTable} produces
a table of significant GO categories, the function \Rfunction{printGenes}
gives significant genes annotated to them.

```{r processtopGOResults, eval=FALSE, dependson="runtopGOTests"}
tab <- GenTable( tgd, Fisher.elim = resultTopGOElim,
		Fisher.classic = resultTopGOClassic,
		orderBy = "Fisher.elim" , topNodes = 100)

gt <- printGenes(tgd, whichTerms = tab$GO.ID
	,chip = "hugene10sttranscriptcluster.db", geneCutOff = 1000)

tab$Siggenes <- sapply(gt, function(x){
		str_c(paste0(x[x$'raw p-value' == 2, "Symbol.id"],";"), collapse = "")
		    })

head(tab[,1:8], 20)
```

A graph of the results can also be produced. Here we visualize the 5 most
significant nodes according to the Fisher elim algorithm in the context of
the GO hierarchy.

```{r  graph of results, eval=FALSE}
pdf("visualizationOfGOResults.pdf", width = 7, height = 7,
    pointsize = 30)
showSigOfNodes(tgd, score(resultTopGOElim), firstSigNodes = 5,
               useInfo = 'def')
dev.off()
```


Gene set enrichment analysis has been and is a field of very extensive
research in bioinformatics. For additional approaches see the `r Biocpkg("topGO") `
vignette and the references therin and also in the [GeneSetEnrichment view](http://bioconductor.org/packages/release/BiocViews.html#___GeneSetEnrichment)
.


# EnrichmentBrowser  based analysis

The `r Biocpkg("EnrichmentBrowser") ` package enables

```{r prepareDataEB, dependson="createDesign"}
#load(file.path("dataMAWorkflow", "PalmieriFinal.RData"))

```


We now remove the raw data directory again

```{r cleanUpRawData, eval = F}
if(dir.exists(rawDataDir)){

    unlink(rawDataDir, recursive = TRUE)
}
```





